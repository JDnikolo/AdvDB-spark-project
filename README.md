# AdvDB-spark-project  
**Ομάδα 1**  
**Νικολόπουλος Ιωάννης**  
**ΑΜ 03115170**  
Κώδικας και σχετικά αρχεία της προαιρετικής εργασίας του μαθήματος "Προχωρημένα Θέματα Βάσεων Δεδομένων". Τα περιεχόμενα του κάθε φακέλου περιγράφονται παρακάτω.  

## Code  
Περιλαμβάνει τον κώδικα που υλοποιεί τα ερωτήματα της εργασίας, καθώς και κώδικα για μέτρηση, αποθήκευση και παρουσίαση των χρόνων εκτέλεσης.
* **import_to_HDFS.sh**: προσθέτει τα δεδομένα διαδρομών και περιοχών (από το directory **../TaxiSource**) στο root directory του HDFS με χρήση shell commands του HDFS, *διαγράφοντας πρώτα όλα τα υπόλοιπα αρχεία .csv και .parquet στο root directory και στο φάκελο /parsedData*. **ΜΗΝ ΤΟ ΧΡΗΣΙΜΟΠΟΙΗΣΕΤΕ ΑΝ ΕΧΕΤΕ ΣΗΜΑΝΤΙΚΑ ΑΡΧΕΙΑ .csv Ή .parquet ΣΤΟ DIRECTORY!** Εμφανίζει τα περιεχόμενα του root directory κατά την ολοκλήρωσή του.   
Κλήση:  
>`./import_to_HDFS.sh`
* **firstParse.py**: διαβάζει τα δεδομένα διαδρομών και περιοχών από το HDFS, ενώνοντάς τα σε ένα dataframe, και τα αποθηκεύει στο directory */parsedData* του HDFS. Μετά τη συνένωση, αφαιρούνται εγγραφές με null τιμές ή αρνητικές αριθμητικές τιμές (στα πεδία που χρησιμοποιούμε στα ερωτήματα). Τα δεδομένα διαδρομών αποθηκεύονται ως taxidata.parquet (ως DataFrame) και ως taxidata (RDD ως Pickle File), ενώ τα δεδομένα περιοχών αποθηκεύονται αντίστοιχα ως zonedata.parquet και zonedata.  
Κλήση: 
>`python firstParse.py [args]`   
(όπου args κάποια από τα: [taxis, locations, rdd, df] για διάβασμα και δημιουργία των αντίστοιχων δεδομένων, ή κενό ή all για εκτέλεση όλων.)
* **queries.py**: ο κώδικας εκτέλεσης των ερωτημάτων της εργασίας. Κάθε ερώτημα υλοποιείται από μια συνάρτηση Python, η οποία επιστρέφει το συνολικό χρόνο εκτέλεσης, το Post-Read χρόνο εκτέλεσης, και το αποτέλεσμα του ερωτήματος. Όλες οι συναρτήσεις δημιουργούν εκ νέου το SparkSession χρησιμοποιώντας τις παραμέτρους που καθορίζονται στην εκφώνηση της εργασίας. Έπίσης, όλες δέχονται μια προαιρετική boolean παράμετρο  standalone (default False), που αν είναι True θα τερματιστεί το SparkSession που δημιουργείται (και συνεπώς το αποτέλεσμα του ερωτήματος δεν θα είναι προσπελάσιμο).   
Χρήση (εντός PySpark ή Python interpreter):  
> `>>>from queries import *`  
> `>>>total,postRead,result = executeQ#()`  
Όπου Q# κάποιο από τα: {Q1, Q2, Q3API, Q3RDD, Q4, Q5}
* **measure_times.py**: κώδικας που εκτελεί όλα τα ερωτήματα σειριακά κάθε 15 λεπτά, αποθηκεύοντας τους χρόνους εκτέλεσης σε αρχεία .csv στο φάκελο time, ανάλογα με τον αριθμό worker που καθορίζεται κατά την κλήση του αρχείου. Ο κώδικας *δεν διαχερίζεται τους workers*, χρησιμοποιεί κάθε φορά όσους καθορίζονται από το Spark.
Χρήση:  
> `python measure_times.py [1worker]`  
Συνίσταται η εκτέλεση στο παρασκήνιο μέσω της εντολής `nohup`.
* **check_times.ipynb**: συνοπτική παρουσίαση των χρόνων εκτέλεσης που συλλέχθηκαν κατά την εκτέλεση των ερωτημάτων (μέσοι χρόνοι, διασπορές γενικές και ανά ώρα εκτέλεσης).  
### Code/time
Περιλαμβάνει σε μορφή csv τους χρόνους εκτέλεσης που μετρήθηκαν, καθώς και τα logs των (τελευταίων) μετρήσεων. Τα αρχεία 1w αφορούν εκτέλεση με 1 worker και τα υπόλοιπα εκτέλεση με 2 workers.

### Taxidata
Τα δεδομένα διαδρομών και περιοχών που χρησιμοποιήθηκαν ως είσοδοι για τα ερωτήματα. Θα πρέπει τα αρχεία να βρίσκονται σε αυτή τη σχετική θέση ως προς το φάκελο Code έτσι ώστε να γίνεται σωστά η εισαγωγή στο HDFS από το αντίστοιχο script.
